---
title: 大模型训练中为什么需要保存中间值
tags:
  - AI
category:
  - AI
author: bsyonline
lede: 没有摘要
date: 2018-05-28 15:32:21
thumbnail:
---



激活值

以一个 2 层 transformer 模型为例：

前向计算第一层的输出结果
$$
\displaylines{
x \cdot W_1=z_1 
}
$$
经过激活函数得到 $a_1$
$$
\displaylines{
simoid(z_1)=a_1
}
$$
由 $a_1$ 乘第二层的权重得到第二层的输出
$$
\displaylines{
a_1 \cdot W_2=z_2
}
$$
通过和真实值 y 求均方差得到 loss
$$
loss=(y-z_2)^2
$$

$z_1$ 、$z_2$ 、$a_1$ 都是计算 loss 的中间过程产生的值。

反向计算 $z_2$ 的梯度

$$
\frac{\partial Loss}{\partial W_2}=\frac{\partial Loss}{\partial z_2}\cdot\frac{\partial z_2}{\partial W_2}=2(\textcolor{#FF0000}{z_2}-y)\cdot\textcolor{#FF0000}{a_1}
$$

反向计算 $z_1$ 的梯度:

$$
\frac{\partial Loss}{\partial W_1}=\frac{\partial Loss}{\partial z_2}\cdot\frac{\partial z_2}{\partial a_2}\cdot\frac{\partial a_1}{\partial z_1}\cdot\frac{\partial z_1}{\partial W_1}=2(\textcolor{#FF0000}{z_2}-y) \cdot W_2\cdot\sigma(\textcolor{#FF0000}{{z_1}})\cdot(1-\sigma(z_1))\cdot x
$$

可见在反向计算梯度时需要用到 $z_1$ 、$z_2$ 、$a_1$ 这几个中间值。