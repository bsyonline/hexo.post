---
title: 强化学习
tags:
  - AI
category:
  - AI
author: bsyonline
lede: 没有摘要
date: 2018-05-28 15:32:21
thumbnail:
---

对于一个分类任务就是将一张图片交给一个分类器，生成不同类别的概率分布。我们对于这张图片有一个标签，来标识它属于哪个类别。用这个标签和预测的概率计算一个损失，然后让这个损失变得最小。通过模型来拟合这个概率分布，让损失最小，这就是监督学习 Supervised Learning 。对于监督学习，机器是知道正确答案的，机器根据题目和答案来学习。还有另一类问题，机器不知道正确答案是什么，人可能也没办法给出正确答案，只能通过机器自己来学习。机器虽然不知道正确答案是什么但是知道什么答案是好的，什么答案是不好的。强化学习 Reinforcement Learning  就可以用来解决这类问题。以围棋为例，agent 就是棋手，围棋的规则就是 envrionment ，棋盘就是 state ，将棋盘交给棋手，棋手可以根据棋盘的状态做出相应的动作就是 action ，做出动作之后会交给环境做出响应，环境可能会根据 action 给 agent 一个奖励 reward ，也可能会生成下一个状态，然后在交给 agent 不断得重复这个过程。强化学习的目的就是如何通过一个策略最大化奖励。

强化学习的数学模型是 Markov Decision Process 。

![[Markov Decision Process.excalidraw.png]]

首先有一个状态 $s_1$ 在这个状态下做出一个行为 $a_1$ 然后交给环境，环境会给一个奖励 $r_1$ 同时会给出下一个状态 $s_2$ ，在下一个状态做出一个行为，然后环境再给一个奖励和下一个状态，依次类推，这个过程就叫做马尔可夫决策过程 Markov Decision Process 。上图中小写的代表实际发生的，我们叫它观测到的，大写的表示未来发生的。

在 $s_1$ 状态下往后产生的所有奖励的和叫做回报。越早得到的奖励价值越高，越往后会有衰减。所以 $s_1$ 的回报就可以表示为

$$
U_1 = r_1+\lambda r_2+...+\lambda^{t-2} r_{t-1}+\lambda^{t-1} R_t+\lambda^t R_{t+1}+...+\lambda^{n-1}R_n
$$

$s_2$ 的回报可以表示为

$$
U_2 = r_2+...+\lambda^{t-3} r_{t-1}+\lambda^{t-2} R_t+\lambda^{t-1} R_{t+1}+...+\lambda^{n-2}R_n
$$

所以 $s_t$ 的回报可以表示为

$$
U_t=R_t+\lambda U_{t+1}
$$

即当前状态的回报就是及时奖励和未来奖励之和。通过回报就可以判断一个状态或者动作的好坏程度。


大语言模型训练中的强化学习

强化学习内容非常多，


| 强化学习              | 大语言模型                                                   |
| ----------------- | ------------------------------------------------------- |
| Policy 策略网络       | 被训练的模型，                                                 |
| Reward Model 奖励模型 | 提前训练好的大语言模型，输入完整的Prompt和回复，输出一个数字，就是Reward，表示这个输入的好坏程度。 |
| Critic Model 价值模型 | 也是一个大语言模型，输入是部分文本，预测一个价值                                |
| State 状态          | t时刻大语言模型输入的token                                        |
| Action 动作         | t时刻大语言模型输出的token                                        |

大语言模型训练使用强化学习就是看一个 Prompt 输出的 token 好不好，好就加以奖励，不好就加以抑制。

强化学习在大语言模型训练中的应用是 OpenAI 提出的，其流程分为四步：
1. 将 Prompt 给要训的模型，生成 10 个输出。
2. 人根据质量高低对 10 个输出进行由高到低排序。
3. 使用标注的数据训练 Reward Model ，使其能够对 Prompt 进行打分。
4. 使用 PPO 算法微调 Policy Model ，使其能够根据 Reward Model 的打分奖励好的输出，抑制不好的输出。