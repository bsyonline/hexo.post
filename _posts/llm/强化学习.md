---
title: 强化学习
tags:
  - AI
category:
  - AI
author: bsyonline
lede: 没有摘要
date: 2018-05-28 15:32:21
thumbnail:
---

强化学习（Reinforcement Learning）是指智能体在复杂环境中最大化奖励从而达到自主决策的目的。强化学习有几个重要的概念：
- agent：智能体，强化学习的主体，也是我们要训练的模型。
- state：状态，对当前环境的概括。所有可能存在的状态集合叫做 state space 。
- environment：环境，与智能体交互的对象。
- action：动作，智能体依据当前状态做出的决策。所有动作的集合叫做 action space 。
- reward：奖励，智能体在做出一个动作之后，环境给智能的返回的一个数值。
- state transition：状态转移， 智能体从当前时刻的状态转移到下一个时刻的状态的过程。

强化学习的模型如下图所示：

![[Reinforce Learning.png]]

### 强化学习和监督学习的区别

对于一个分类任务就是将一张图片交给一个分类器，生成不同类别的概率分布。我们对于这张图片有一个标签，来标识它属于哪个类别。用这个标签和预测的概率计算一个损失，然后让这个损失变得最小。通过模型来拟合这个概率分布，让损失最小，这就是监督学习 Supervised Learning 。对于监督学习，机器是知道正确答案的，机器根据题目和答案来学习。

还有另一类问题，机器不知道正确答案是什么，人可能也没办法给出正确答案，只能通过机器自己来学习。机器虽然不知道正确答案是什么但是知道什么答案是好的，什么答案是不好的。强化学习 Reinforcement Learning  就可以用来解决这类问题。以围棋为例，agent 就是棋手，围棋的规则就是 envrionment ，棋盘就是 state ，将棋盘交给棋手，棋手可以根据棋盘的状态做出相应的动作就是 action ，做出动作之后会交给环境做出响应，环境可能会根据 action 给 agent 一个奖励 reward ，也可能会生成下一个状态，然后在交给 agent 不断得重复这个过程。强化学习的目的就是如何通过一个策略最大化奖励。


### 强化学习的分类

![[reinforcement learning framework.png]]


### Markov Decision Process

强化学习的数学模型是 Markov Decision Process 。

![[Markov Decision Process.excalidraw.png]]

首先有一个状态 $s_1$ 在这个状态下做出一个行为 $a_1$ 然后交给环境，环境会给一个奖励 $r_1$ 同时会给出下一个状态 $s_2$ ，在下一个状态做出一个行为，然后环境再给一个奖励和下一个状态，依次类推，这个过程就叫做马尔可夫决策过程 Markov Decision Process 。上图中小写的代表实际发生的，我们叫它观测到的，大写的表示未来发生的。

在 $s_1$ 状态下往后产生的所有奖励的和叫做回报。越早得到的奖励价值越高，越往后会有衰减。所以 $s_1$ 的回报就可以表示为

$$
U_1 = r_1+\lambda r_2+...+\lambda^{t-2} r_{t-1}+\lambda^{t-1} R_t+\lambda^t R_{t+1}+...+\lambda^{n-1}R_n
$$

$s_2$ 的回报可以表示为

$$
U_2 = r_2+...+\lambda^{t-3} r_{t-1}+\lambda^{t-2} R_t+\lambda^{t-1} R_{t+1}+...+\lambda^{n-2}R_n
$$

所以 $s_t$ 的回报可以表示为

$$
U_t=R_t+\lambda U_{t+1}
$$

即当前状态的回报就是及时奖励和未来奖励之和。通过回报就可以判断一个状态或者动作的好坏程度。


求解马尔可夫决策过程中的价值函数的方法就包括上边图中的蒙特卡洛方法（Monte Carlo）和时序差分（Temporal Difference）。


### 蒙特卡洛方法


### 时序差分



### 梯度策略

策略梯度是一种强化学习方法，目标是让智能体通过“试错”调整自己的策略。梯度增加是策略梯度的一种优化方法，即沿着梯度方向调整参数，使目标函数（如奖励）最大化。比如上山每步都想上走。但是这样做有一个缺点，就是上山的步子太大（策略更新不稳定），容易掉下悬崖（学废了）。所以就有了 TRPO（Trust Region Policy Optimization）。


### TRPO

TRPO 的核心思想就是策略更新的幅度不要太大，避免策略学废了。TRPO 通过 KL散度限制更新的范围（新旧策略不会差异太大）。

$$
\max_{\theta} \mathbb{E}_{t}\left[\frac{\pi_{\theta}(a_t|s_t)}{\pi_{\theta_{\text{old}}}(a_t|s_t)} \hat{A}_t\right] \\
\text{s.t. } \mathbb{E}_{t}\left[\text{KL}[\pi_{\theta_{\text{old}}}(\cdot|s_t) \| \pi_{\theta}(\cdot|s_t)]\right] \leq \delta
$$

TRPO 虽然理论严谨，但是计算复杂，所以有了 TRPO 的简化版 PPO 。


### PPO

PPO (Proximal Policy Optimization) 是 OpenAI 提出的一种强化学习算法。它通过裁剪机制来限制策略更新的幅度，从而控制新旧策略之间的差异。TRPO 好比在学习过程中老师全程监督按照计划分秒不差的实施，PPO 则是老师隔一段时间提醒你不要好高骛远，要把基础打扎实。PPO 比 TRPO 实行起来简单很多，而且效果也不差。所以 PPO 因其平衡了训练效率和效果，成为了大部分强化学习的首选。

$$
L_t^{\text{PPO}}(\theta) = \mathbb{E}_t \left[ L_t^{\text{CLIP}}(\theta) - c_1 L_t^{\text{VF}}(\theta) + c_2 S[\pi_\theta](s_t) \right]
$$

- 策略损失（CLIP）： $L_t^{\text{CLIP}}(\theta) = \min\left( r_t(\theta) \hat{A}^{GAE}_t, \text{clip}(r_t(\theta), 1-\epsilon, 1+\epsilon) \hat{A}_t \right)$

- 价值函数误差（MSE）： $L_t^{\text{VF}}(\theta) = (V_\theta(s_t) - V_t^{\text{targ}})^2$

- 策略熵（探索奖励）： $S[\pi_\theta](s_t) = -\sum_{a} \pi_\theta(a|s_t) \log \pi_\theta(a|s_t)$

- 优势函数估计（GAE）：$\hat{A}_t^{\text{GAE}} = \sum_{l=0}^{T-t+1} (\gamma \lambda)^l \delta_{t+l}$

- TD 误差： $\delta_t = r_t + \gamma V(s_{t+1}) - V(s_t)$


![[ppo.png]]
PPO 使用了 4 个模型，Value Model 通常与 Policy Model 大小相当，所以带来了巨大的计算和内存的负担。

### GRPO

GRPO （Group Relative Policy Optimization）是 DeepSeek 提出的一种优化算法。去掉了 Value Model ，采用多次采样的平均奖励作为 baseline ，显著减少了内存占用。

![[grpo.png]]


### DPO


### GSPO





### 大语言模型训练中的强化学习




| 强化学习              | 大语言模型训练                                                 |
| ----------------- | ------------------------------------------------------- |
| Policy 策略网络       | 被训练的模型                                                  |
| Reward Model 奖励模型 | 提前训练好的大语言模型，输入完整的Prompt和回复，输出一个数字，就是Reward，表示这个输入的好坏程度。 |
| Critic Model 价值模型 | 也是一个大语言模型，输入是部分文本，预测一个价值                                |
| State 状态          | t时刻大语言模型输入的token                                        |
| Action 动作         | t时刻大语言模型输出的token                                        |

大语言模型训练使用强化学习就是看一个 Prompt 输出的 token 好不好，好就加以奖励，不好就加以抑制。

强化学习在大语言模型训练中的应用是 OpenAI 提出的，其流程分为 3 步：
1. 将 Prompt 给要训的模型，生成 10 个输出。人根据质量高低对 10 个输出进行由高到低排序。
2. 使用标注的数据训练 Reward Model ，使其能够对 Prompt 进行打分。
3. 使用强化学习算法微调 Policy Model ，使其能够根据 Reward Model 的打分奖励好的输出，抑制不好的输出。

![[openai-ppo.png]]
