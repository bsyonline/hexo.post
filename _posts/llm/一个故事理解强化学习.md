---
title: 一个故事理解强化学习
tags:
  - AI
category:
  - AI
author: bsyonline
lede: 没有摘要
date: 2018-05-28 15:32:21
thumbnail:
---

小帅和小美是俩兄妹，他们每天去学校上学。

在学校，老师每天上课都会先讲解课程的核心知识，这样小帅和小美可以学习到基础概念和知识。**这个过程可以类比大模型训练的预训练的过程。** 

老师会在课上讲解例题，这些例题有题目有答案，老师还会讲解如何解出正确答案。然后还有课后习题作为作业让小帅和小美进行练习。**这个过程可以类比大模型训练的监督微调的过程。** 

等学习一段时间之后会进行考试，考试的时候是不给答案的，老师会根据答题进行评分，并对试卷进行讲解并让小帅和小美进行改正。**这个过程可以类比大模型训练的强化学习的过程。**

为了鼓励小帅和小美，爸爸定了一条规则：考试低于 80 分罚 10 元，高于 80 分奖励 100 元，高于 90 分奖励 200 元。**这就可以类比强化学习中的奖励。**

小帅成绩不好每次只能考 30 分，小美成绩好每次都能考 90 分，过了一段时间之后，这个激励措施对小帅和小美都失去了作用，因为小帅再怎么努力都考不到 80 分，小美不用怎么努力就能考 90 分。爸爸发现了这个一刀切的奖励机制是有问题的，于是修改了奖励机制：小帅高于 50 分奖励 100 元，高于 70 分奖励 200 元。小美高于 90 分奖励 100元，高于 95 分奖励 200 元。**这种根据不同情况制定奖励策略可以类比强化学习中根据模型状态制定奖励策略。**

这样的奖励机制比较合理，但是随着小帅和小美学习进步，奖励规则不能一成不变。爸爸需要持续的关注小帅和小美的学习情况来修改奖励规则，时间一长，爸爸有点吃不消了，于是爸爸想了一个办法，给小帅和小美请了一个家教，让家教来负责小帅和小美的学习并给出奖励的规则。**这就可以类比 PPO 算法中的 Critic Model （评论家模型） 或 Value Function （价值函数） 。**

这个办法虽然很有效，但是很烧钱。于是爸爸又想了一个办法。在每次考试之前，让小帅和小美做 5 套模拟题，根据 5 套试卷的平均分来制定奖励标准。**这个就可以类比 GRPO ，去掉了 PPO 的 Critic Model ，使用多次采样来制定奖励标准。**

又过了一段时间，爸爸发现每次考试小帅都能考 100 分，爸爸一问才知道，小帅每次考试前都提前偷看了试卷。这是作弊，肯定不行的，于是爸爸又想了一个办法，增加了一条规则，如果考试分数和模拟考分数相差太大，就不能获得奖励。**这个可以类比强化学习中的 KL 散度，防止新模型（策略）和旧模型（参考模型）差异过大。**

