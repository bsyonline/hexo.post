---
title: 大模型训练的显存计算
tags:
  - AI
category:
  - AI
author: bsyonline
lede: 没有摘要
date: 2018-05-28 15:32:21
thumbnail:
---

显存占用：

输入输出
FP32 浮点数的内存占用为 32Bits = 4Bytes ，FP16 浮点数的内存占用为 16Bits = 2Bytes ，BF16 同 FP16 。
当使用 BF16 训练时，第一个 sample 数据经过 tokenizer 后进入第一个 tramsformer block 的大小为：

$$\displaylines{ 
embaddingInputSize = b*s*h*2/1024/1024 \\
b: batch\ size \\
s: seq\ lenth \\
h: hidden\ size \\
}
$$


以 LLama-13B 为例：
batchSize=1
seqLen=1024
hiddenSize=5120

$$
embaddingInputSize=1*1024*5120*2/1024/1024=10MB
$$
输入输出的形状一样，所以总大小为2倍的输入占用的显存大小，为 20MB

模型参数

$$
\displaylines{
1B=1000^3 \\
1GB=1024^3\ Byte \\
故 1B\approx1\ GB
}
$$
以LLama-13B 为例，BF16 精度下模型占用的显存大小为：

$$
13GB*2=26\ GB
$$


优化器
以 Adam 优化器为例，Adam 优化器会存储 2 个值：动量和方差，这2个值是以 float32 精度存储的。此外优化器还会以 float32 精度存储一份模型参数用于梯度更新，故 Adma 的显存占用为：

$$
Adam优化器显存占用=3*13\ GB*4=156\ GB
$$

激活值


$$
\displaylines{
激活值显存占用 = s \cdot b \cdot h \cdot (34 + 5 \cdot a \cdot s / h) \cdot L /1024/1024/1024 \\
b: batch\ size \\
s: seq\ lenth \\
h: hidden\ size \\
a: attention\ head\ num \\
L: transformer\ block\ layers\ num
}
$$

 LLama-13B 的激活值显存占用为:

$$\displaylines{
1024*1*5120*(34+5*40*1024/5120)*40/1024/1024/1024=14.5\ GB
}
$$

梯度

每个参数都有一个梯度值，用 float16 存储，所以梯度的显存占用为：

$$
13\ GB * 2=26\ GB
$$

LLama-13B 模型总的显存占用为：
$$\displaylines{
10MB+26GB+156GB+14.5GB+26GB \approx 222.5GB
}
$$
