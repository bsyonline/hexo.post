---
title: 大模型训练的显存计算
tags:
  - AI
category:
  - AI
author: bsyonline
lede: 没有摘要
date: 2018-05-28 15:32:21
thumbnail:
---
## 什么会占显存

### 输入输出
FP32 浮点数的内存占用为 32Bits = 4Bytes ，FP16 浮点数的内存占用为 16Bits = 2Bytes ，BF16 同 FP16 。
当使用 BF16 训练时，第一个 sample 数据经过 tokenizer 后进入第一个 tramsformer block 的大小为：

$$\displaylines{ 
embaddingInputSize = b*s*h*2/1024/1024 \\
b: batch\ size \\
s: seq\ lenth \\
h: hidden\ size \\
}
$$


以 LLama-13B 为例：
batchSize=1
seqLen=1024
hiddenSize=5120

$$
embaddingInputSize=1*1024*5120*2/1024/1024=10MB
$$
输入输出的形状一样，所以总大小为2倍的输入占用的显存大小，为 20MB

### 模型参数

$$
\displaylines{
1B=1000^3 \\
\\
1GB=1024^3\ Byte \\
\\
故 1B\approx1\ GB
}
$$

以LLama-13B 为例，BF16 精度下模型占用的显存大小为：

$$
13GB*2=26\ GB
$$

### 梯度

每个参数都有一个梯度值，所以梯度占用的显存和模型的参数量相关，用 float16 存储，所以梯度的显存占用为：

$$
13\ GB * 2=26\ GB
$$

LLama-13B 模型总的显存占用为：

$$\displaylines{
10MB+26GB+156GB+14.5GB+26GB \approx 222.5GB
}
$$

### 优化器
以 Adam 优化器为例，Adam 优化器会存储 2 个值：动量和方差，这2个值是以 float32 精度存储的。此外优化器还会以 float32 精度存储一份模型参数用于梯度更新，故 Adma 的显存占用为：

$$
Adam优化器显存占用=3*13\ GB*4=156\ GB
$$

### 激活值

#### 什么是激活值
激活(activations)是指前向传递过程中计算得到，并在后向传递过程中需要用到的所有张量。

激活值的显存占用包括 2 部分：**Attention block 的显存占用**和 **MLP block的显存占用**。
符号说明：
- a: number of attention heads
- b: microbatch size
- h: hidden size
- L: number of transformer layers
- p: pipeline parallel size
- s: sequence length
- t: tensor parallel size
- v: vocabulary size
#### Attention block 的显存占用

$Q,K,V$ 的计算公式为

$$
Q=xW_q
$$

$$
K=xW_k
$$

$$
V=xW_V
$$

self attention 的计算公式为

$$
\displaylines{
Attention=Softmax(\frac{QK^T}{\sqrt{d_k}})V \cdot W_o + x
}
$$

故计算过程占用的显存如下表

| 计算步骤                  | 中间激活         | 形状                         | 占用显存            |
| :-------------------- | :----------- | :------------------------- | :-------------- |
| $Q/K/V=xW_{q/k/v}$    | $x$          | [b,s,h]                    | $2bsh$          |
| $QK^{T}$              | $Q,K$        | [b,a,s,h/a]<br>[b,a,h/a,s] | $2 \cdot 2bsh$  |
| $score=softmax(QK^T)$ | $QK^T$       | [b,a,s,s]                  | $2bs^2a$        |
| $dropout()$           | dropout_mask | [b,a,s,s],int              | $bs^2a$         |
| $x=score \cdot V$     | $scroe$,V    | [b,a,s,s]<br>[b,a,h/a,s]   | $2bs^2a + 2bsh$ |
| $xW_o$                | $x$          | [b,a,s,h/a]                | $2bsh$          |
| $dropout()$           | dropout_mask | [b,s,h],int                | $bsh$           |
| $layernorm()$         | $x$          | [b,s,h]                    | $2bsh$          |
| SUM                   |              |                            | $13bsh+5bs^2a$  |
#### MLP block 的显存占用

MLP block 的计算公式为

$$
x=f_{gelu}(x_{out}W_1)W_2+x_{out}
$$

故计算过程占用的显存如下表

| 计算步骤                | 中间激活         | 形状          | 占用显存    |
| :------------------ | :----------- | :---------- | :------ |
| $xW_{up}$           | $x$          | [b,s,h]     | $2bsh$  |
| $f_{gelu}(xW_{up})$ | $xW_{up}$    | [b,s,4h]    | $8bsh$  |
| $xW_{down}$         | $x$          | [b,s,4h]    | $8bsh$  |
| $dropout()$         | dropout_mask | [b,s,h],int | $bsh$   |
| $leyernorm()$       | $x$          | [b,s,h]     | $2bsh$  |
| SUM                 |              |             | $21bsh$ |


综上，Attention block 和 MLP block的显存占用总和为

$$\displaylines{
(34bsh+5bs^2a) \cdot L\\
\\
=bsh(34+\frac{5sa}{h})\cdot L
}
$$

 LLama-13B 的激活值显存占用为:

$$\displaylines{
1024*1*5120*(34+5*40*1024/5120)*40/1024/1024/1024=14.5\ GB
}
$$


## 显存优化方案

通过上边的分析在大模型训练过程中显存占用非常巨大，随着模型的参数的增加，占用的显存远远超过了 GPU 的显存，所以就需要有一些其他的手段来降低显存占用。优化的手段就是用时间换空间。

### 3D 并行

#### 模型参数

bf16 精度下，模型训练 tp 和 pp 切分后显存占用为

$$
\frac{2 \cdot modelSize}{t \cdot p}
$$
#### 梯度

同模型参数。

#### 激活值

3D 并行对于激活值显存占用来说，不用考虑 PP 只考虑 TP 即可。因为 Pipeline Parallel 并行度为 p ，整个 PP 会有 $L/p$ 个 microbatch 。虽然参数只有 $L/p$  但是激活状态需要整个 batch 开始反向，所以对于第一个 stage 来说，PP 并没有减少显存占用。

| 计算步骤                  | 中间激活         | 形状                         | 占用显存            | 是否切分 |
| :-------------------- | :----------- | :------------------------- | :-------------- | ---- |
| $Q/K/V=xW_{q/k/v}$    | $x$          | [b,s,h]                    | $2bsh$          | 不切分  |
| $QK^{T}$              | $Q,K$        | [b,a,s,h/a]<br>[b,a,h/a,s] | $2 \cdot 2bsh$  |      |
| $score=softmax(QK^T)$ | $QK^T$       | [b,a,s,s]                  | $2bs^2a$        |      |
| $dropout()$           | dropout_mask | [b,a,s,s],int              | $bs^2a$         |      |
| $x=score \cdot V$     | $scroe$,V    | [b,a,s,s]<br>[b,a,h/a,s]   | $2bs^2a + 2bsh$ |      |
| $xW_o$                | $x$          | [b,a,s,h/a]                | $2bsh$          | 不切分  |
| $dropout()$           | dropout_mask | [b,s,h],int                | $bsh$           | 不切分  |
| $layernorm()$         | $x$          | [b,s,h]                    | $2bsh$          | 不切分  |
| SUM                   |              |                            | $13bsh+5bs^2a$  |      |
| $xW_{up}$             | $x$          | [b,s,h]                    | $2bsh$          |      |
| $f_{gelu}(xW_{up})$   | $xW_{up}$    | [b,s,4h]                   | $8bsh$          |      |
| $xW_{down}$           | $x$          | [b,s,4h]                   | $8bsh$          |      |
| $dropout()$           | dropout_mask | [b,s,h],int                | $bsh$           | 不切分  |
| $leyernorm()$         | $x$          | [b,s,h]                    | $2bsh$          | 不切分  |
| SUM                   |              |                            | $21bsh$         |      |
如果开了 TP ，Attention/MLP 的 Input 、Dropout 以及 Leyernorm 不会被切分，总共 $10bsh$ ，其他部分都会被切分。

$$
\displaylines{
(10bsh+\frac{24bsh}{t}+\frac{5bs^2a}{t}) \cdot L/1024^3\\
\\
=bsh(10+\frac{24}{t}+\frac{5sa}{h \cdot t}) \cdot L / 1024^3
}
$$

如果开了 SP ，会对 Input 、Dropout 以及 Leyernorm 进行切分

$$
\displaylines{
bsh(10+\frac{24}{t}+\frac{5sa}{h \cdot t}) \cdot L\\
\\
=bsh(\frac{10}{t}+\frac{24}{t}+\frac{5sa}{h \cdot t}) \cdot L\\
\\
=bsh(\frac{34}{t}+\frac{5sa}{h\cdot t})\cdot L
}
$$



### ZeRO

ZeRO（Zero Redundancy Optimizer）是一种用于分布式深度学习训练的优化技术，核心思想是通过对模型状态进行分割（partition）来代替对模型状态的复制来减少内存冗余，说白了就是只存一份模型状态，需要的时候通过通信来进行同步。
在大模型训练过程中，DP 通信效率高但是不会降低模型内存占用，MP 能降低模型内存占用但是会带来通信开销。ZeRO-DP（ZeRO-powered data parallelism）是一种 ZeRO 的实现，既是现实了 DP 的通信效率，同时实现了 MP 的内存效率。ZeRO-DP 有三个优化阶段，分别对应对 Model States 不同程度的分割：
- ZeRO-1：分割 `Optimizer States`，
- ZeRO-2：分割 `Optimizer States` 与 `Gradients`
- ZeRO-3：分割 `Optimizer States` 、`Gradients` 与 `Parameters`

![[ZeRO.png]]
从图上的实验数据可以看出，ZeRO-1 能够减少 4 倍的显存占用，通信量与 DP 相同。ZeRO-2 能够减少 8 倍的显存占用，通信量与 DP 相同。ZeRO-3 减少的显存和 N 呈线性关系，通信量较 DP 增加 50%（因为前向和反向都需要 AllGather 一次参数）。

### 重计算

重计算是一种优化激活值显存占用的技术。
- 全量重计算：不保存中间结果（激活检查点），每次都从输入重新进行计算。这种方式可以大幅降低显存占用，但是会增加 30%-40% 的计算时间。
- 选择重计算：
	- 选择保存部分 Transformer 层的中间结果。由于模型的层数导致不好均匀切分，扩展性不好。
	- 不是对 Transformer 层的中间结果进行保存，而是对每个层占用显存大但是计算又不复杂的部分进行重计算。

在激活值的计算过程中

$$\displaylines{
bsh(34+\frac{5sa}{h})\cdot L
}
$$

通过线性层计算出 $Q,K,V$ 向量后，所进行的一系列运算（如 $QK^T$ 矩阵乘法、Softmax、Softmax Dropout，以及对 $V$ 的加权求和等）会显著增大张量宽度，正是这部分操作产生了大量的中间激活值，这些激活值输入维度很大，占用内存高，但每个元素所需的浮点计算却很少。相比之下，Transformer 层的其余部分（主要是前馈网络等）对应于公式中的 34 这一项，其激活占用相对较小但计算密集。所以对 $\frac{5sa}{h}$ 使用选择重计算，显存占用为

$$\displaylines{
bsh(34)\cdot L
}
$$

#### Megatron 中重计算的使用

以 Megatron-LM 为例，开启全量重计算使用 `--recompute-granularity full`，开启选择重计算使用 `--recompute-activations` 。

##### 全量重计算

重计算粒度设置为 full，表明在前向计算过程中，只保留每个 layer 组的 input 数据，其他所有数据都通过计算获得。重计算方法分为以下 block 和 uniform 两种。
- **--recompute-method block --recompute-num-layers 14**：block 重计算，重计算前 {recompute-num-layers} 层的 Transformer block，当 PP 并行大于 1 时，每个 PP rank 只处理 L/p 层 layer，这里的指 L/p 层 layer 里的前 {recompute-num-layers} 层进行激活重计算，如QWen2.5-7B 的 layer=28，当 p=2 时，若要开启全部层数全量重计算则设置 {recompute-num-layers}=14 即可。
- **--recompute-method uniform --recompute-num-layers 1**：uniform 重计算，将 L/p 层分成L/p/{recompute-num-layers} 个 block 分组，重计算每个 block 分组的第一个 layer 进行全量重计算，只保留 input 输入。

##### 选择重计算

开启 {recompute-activations} 激活重计算参数，对于每一个 Transformer block 中的 self attention block 进行激活重计算，只保留 attention block 中的 input 输入。

#### 激活函数重计算


### 分布式优化器

通过 `--use-distributed-optimizer` 参数来控制开关。将训练中的优化器状态均匀的切分到不同**DP** 组的 rank 上，切分力度相当于 deepspeed 的 **Zero-1** 。



### Flash Attention

FlashAttention 是一种高效的注意力机制实现方法，专门针对 Transformer 模型中的自注意力（Self-Attention）计算进行优化。FlashAttention 旨在减少自注意力计算中的内存占用和计算开销，同时保持模型的性能。FlashAttention 的核心思想是分块计算（Tiling）和重计算（Recomputation）。

#### FlashAttention1

GPU 的内存分为 SRAM 、BHM 和 DRAM ，依次速度降低，容量增大。

![[gpu内存分布.png]]
标准的 attention 实现过程如下：
1. 从 HBM 中 读取 $Q,K \in R^{N\times d}$ ，计算 $S=QK^T \in R^{N \times N}$ ，然后将 $S$ 写入 HBM 。
2. 从 HBM 中 读取 $S$ ，计算 $P=softmax(S) \in R^{N \times N}$ ，然后将 $P$ 写入 HBM 。
3. 从 HBM 中 读取 $P,V$ ，计算 $O=PV \in R^{N \times d}$ ，然后将 O 写入 HBM。
4. 返回 $O \in R^{N \times d}$

整个过程从 HBM 读写了 3 次 block 矩阵，IO 复杂度则是 $O(Nd+N^2)$。通常 $N\gg d$ ，所以复杂度可以认为是 $O(N^2)$ 。

由于 SRAM 容量限制，FlashAttention 将输入的 $Q,K,V$ 进行分块（tiles），逐块从慢速的 HBM 加载到快速的 SRAM 中计算并累加结果。在前向传播过程中，只保存必要的中间结果，而在反向传播时重新计算需要的中间值。

![[flashattention.png]]

![[flashattention algo.png]]

FlashAttention IO 复杂度则是 $O(Nd*Nd/M)$。由于 $d \leq M\leq Nd$ ，所以复杂度可以认为是 $O(Nd)$ 。FlashAttention 相比于标准算法，优化了计算方式，虽然运算量没有减少，但在运算过程中避免了 S 和 P 这种 $N\times N$ 大矩阵在 HBM 中的读写，从而提高了效率。


#### FlashAttention2

相比于 Flashattention1，FlashAttention2 总的来说就是减少了非矩阵乘法计算，优化并行效率，采用共享存储，减少通信，具体如下：
1. FlashAttention2 调整了 $Q,K,V$ 的循环计算的方式，无需再通信 $K,V$ 分块中产生的 $m,l$ 中间值，而是使用 $L=m+logl$ 来代替，减少了通信次数并且节省了激活显存。
2. FlashAttention2 减少了非矩阵乘法的计算，在 Flashattention1 中，计算 $O$ 时存在非矩阵乘法计算，在 Flashattention2 中对其进行改造，在循环过程中，不对其进行元素缩放，记录统计值 $l$ ，只在最后一个 loop 的时候再对其进行缩放，得到正确的 $O$ 。
3. 对于 LLM 中的 Decode-Only 可以完全忽略上三角 block 的计算，Flashattention2 跳过了需要完全 mask 的block计算。
4. Flashattention1，在 $K,V$ 分块中划分线程组，线程组之间共享 $Q$ ，线程组计算的中间值  $m、l、o$ 需要写到 HBM 中，因此效率较低。在 FlashAttention2 中，在 $Q$ 分块中划分线程组，线程组之间共享 $K,V$ ，因此线程组之间无需通信中间值。
5. 优化并行：相比于 Flashattention1 ，FlashAttention2 在 batch-size 和 head-nums 上进行并行策略， 考虑到目前的长序列场景，FlashAttention2 在 Flashattention1 的并行策略的基础上，在外层对序列 seq 进行并行。



### 总结

| 优化策略            | 显存占用                                               |
| --------------- | -------------------------------------------------- |
| 没有优化            | $bsh(34+\frac{5sa}{h})\cdot L$                     |
| TP              | $bsh(10+\frac{24}{t}+\frac{5sa}{h\cdot t})\cdot L$ |
| SP+TP           | $bsh(\frac{34}{t}+\frac{5sa}{h\cdot t})\cdot L$    |
| 全量重计算           | $bsh(2)\cdot L$                                    |
| 选择重计算           | $bsh(34)\cdot L$                                   |
| TP+选择重计算        | $bsh(10+\frac{24}{t})\cdot L$                      |
| SP+TP+选择重计算     | $bsh(\frac{34}{t})\cdot L$                         |
| Flash Attention |                                                    |
