---
title: 论文合集
date: 2017-05-30 10:22:41
tags:
  - llm
  - AI
category:
  - AI
thumbnail: 
author: bsyonline
lede: 没有摘要
---

[Attention Is All You Need](https://arxiv.org/pdf/1706.03762)

[On Layer Normalization in the Transformer Architecture](https://arxiv.org/pdf/2002.04745)

[Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism](https://arxiv.org/pdf/1909.08053)

[ZeRO: Memory Optimizations Toward Training Trillion Parameter Models](https://arxiv.org/pdf/1910.02054)

[Reducing Activation Recomputation in Large Transformer Models](https://arxiv.org/pdf/2205.05198)

[FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness](https://arxiv.org/pdf/2205.14135)

[FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning](https://arxiv.org/pdf/2307.08691)

[From Online Softmax to FlashAttention](https://courses.cs.washington.edu/courses/cse599m/23sp/notes/flashattn.pdf)

[Rethinking and Improving Natural Language Generation with Layer-Wise Multi-View Decoding](https://arxiv.org/pdf/2005.08081)

[PREPRINT ACCEPTED FOR PUBLICATION IN SCIENTOMETRICS](https://arxiv.org/pdf/1506.03009)

[Scheduled Sampling for Transformers](https://arxiv.org/pdf/1906.07651)

[PARALLEL SCHEDULED SAMPLING](https://arxiv.org/pdf/1906.04331)

[SEQUENCE LEVEL TRAINING WITH RECURRENT NEURAL NETWORKS](https://arxiv.org/pdf/1511.06732)

[Proximal Policy Optimization Algorithms](https://arxiv.org/pdf/1707.06347)

[Sequence Parallelism: Long Sequence Training from System Perspective](https://arxiv.org/pdf/2105.13120)

[Reducing Activation Recomputation in Large Transformer Models](https://arxiv.org/pdf/2205.05198)

[Efficient Estimation of Word Representations in Vector Space](https://arxiv.org/pdf/1301.3781))

[Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks](https://arxiv.org/pdf/1908.10084)

