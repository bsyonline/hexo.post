---
title: 论文合集
date: 2017-05-30 10:22:41
tags:
  - llm
  - AI
category:
  - AI
thumbnail: 
author: bsyonline
lede: 没有摘要
---
### 架构

| 论文                                                                                         | 说明          |
| ------------------------------------------------------------------------------------------ | ----------- |
| [Attention Is All You Need](https://arxiv.org/pdf/1706.03762)                              | 介绍自注意力机制    |
| [On Layer Normalization in the Transformer Architecture](https://arxiv.org/pdf/2002.04745) | 介绍归一化层位置和作用 |
| [Scheduled Sampling for Transformers](https://arxiv.org/pdf/1906.07651)                    |             |


### 大模型训练与并行技术


| 论文                                                                                                                        | 说明                                              |
| ------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------- |
| [Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism](https://arxiv.org/pdf/1909.08053) | 介绍模型并行                                          |
| [ZeRO: Memory Optimizations Toward Training Trillion Parameter Models](https://arxiv.org/pdf/1910.02054)                  | 内存优化框架ZeRO                                      |
| [Sequence Parallelism: Long Sequence Training from System Perspective](https://arxiv.org/pdf/2105.13120)                  | 序列并行                                            |
| [Reducing Activation Recomputation in Large Transformer Models](https://arxiv.org/pdf/2205.05198)                         | 激活重计算                                           |
| [FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness](https://arxiv.org/pdf/2205.14135)           | FlashAttention算法                                |
| [FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning](https://arxiv.org/pdf/2307.08691)      | FlashAttention2算法                               |
| [From Online Softmax to FlashAttention](https://courses.cs.washington.edu/courses/cse599m/23sp/notes/flashattn.pdf)       | 从理论层面梳理 “在线 Softmax” 到 “FlashAttention” 的技术演进逻辑 |


NLP

| 论文                                                                                                                           | 说明  |
| ---------------------------------------------------------------------------------------------------------------------------- | --- |
| [Rethinking and Improving Natural Language Generation with Layer-Wise Multi-View Decoding](https://arxiv.org/pdf/2005.08081) |     |
| [PREPRINT ACCEPTED FOR PUBLICATION IN SCIENTOMETRICS](https://arxiv.org/pdf/1506.03009)                                      |     |
| [PARALLEL SCHEDULED SAMPLING](https://arxiv.org/pdf/1906.04331)                                                              |     |
| [Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks](https://arxiv.org/pdf/1908.10084)                           |     |
| [Efficient Estimation of Word Representations in Vector Space](https://arxiv.org/pdf/1301.3781)                              |     |
| [SEQUENCE LEVEL TRAINING WITH RECURRENT NEURAL NETWORKS](https://arxiv.org/pdf/1511.06732)                                   |     |


### 强化学习


| 论文                                                                          | 说明  |
| --------------------------------------------------------------------------- | --- |
| [Proximal Policy Optimization Algorithms](https://arxiv.org/pdf/1707.06347) |     |


























