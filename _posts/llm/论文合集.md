---
title: 论文合集
date: 2017-05-30 10:22:41
tags:
  - llm
  - AI
category:
  - AI
thumbnail: 
author: bsyonline
lede: 没有摘要
---

[Attention Is All You Need](https://arxiv.org/pdf/1706.03762)

[On Layer Normalization in the Transformer Architecture](https://arxiv.org/pdf/2002.04745)

[Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism](https://arxiv.org/pdf/1909.08053)

[ZeRO: Memory Optimizations Toward Training Trillion Parameter Models](https://arxiv.org/pdf/1910.02054)

[Reducing Activation Recomputation in Large Transformer Models](https://arxiv.org/pdf/2205.05198)
