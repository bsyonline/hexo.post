---
title: 论文合集
date: 2017-05-30 10:22:41
tags:
  - llm
  - AI
category:
  - AI
thumbnail: 
author: bsyonline
lede: 没有摘要
---

[Attention Is All You Need](https://arxiv.org/pdf/1706.03762)

[On Layer Normalization in the Transformer Architecture](https://arxiv.org/pdf/2002.04745)

[Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism](https://arxiv.org/pdf/1909.08053)

[ZeRO: Memory Optimizations Toward Training Trillion Parameter Models](https://arxiv.org/pdf/1910.02054)

[Reducing Activation Recomputation in Large Transformer Models](https://arxiv.org/pdf/2205.05198)

[FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness](https://arxiv.org/pdf/2205.14135)

[FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning](https://arxiv.org/pdf/2307.08691)

[From Online Softmax to FlashAttention](https://courses.cs.washington.edu/courses/cse599m/23sp/notes/flashattn.pdf)
