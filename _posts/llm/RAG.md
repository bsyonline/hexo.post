---
title: RAG
tags:
  - AI
category:
  - AI
author: bsyonline
lede: 没有摘要
date: 2018-05-28 15:32:21
thumbnail:
---


### 向量

向量是同时包含**大小**和**方向**的量。

![[向量.png]]

#### 向量的运算

##### 加

假设有两个向量 $\vec{AB}$ 和 $\vec{CD}$ ，则 $\vec{AB}$ + $\vec{CD}$ 

三角形法则：**首尾相连**，将任意一个向量的起点，**平移**到另一个向量的终点处。

![[向量三角形法则.png]]


平行四边形法则：**起点相连**，将两个向量的起点平移到一起，做出平行四边形。

![[向量平行四边形法则.png]]

##### 减

假设有两个向量 $\vec{AB}$ 和 $\vec{CD}$ 

方法一：$\vec{AB}$ + $\vec{CD}$ = $\vec{AB} + \vec{(-CD)}$ 

![[向量的减法1.png]]
方法二：减向量指向被减向量。

![[向量的减法2.png]]

##### 数乘

向量的数乘将得到一个新向量，方向不变，长度是原向量的 n 倍。

##### 点乘（内积）

两个向量对应位置上的值相乘再相加的操作，其结果即为点积。

假设有两个向量 $\vec{AB}=\begin{bmatrix} x_1 \\ y_1 \end{bmatrix}$ 和 $\vec{CD}=\begin{bmatrix} x_2 \\ y_2 \end{bmatrix}$ ，则 $x_1 \cdot x_2 + y_1 \cdot y_2$ 。

从几何角度看，点积是两个向量的长度与它们夹角余弦的积。

![[向量点乘.png]]

##### 叉乘（外积）

**叉乘（Cross Product）** 又称**向量积**（Vector Product）。




### Embedding

Embedding 是将高维的、离散的、非结构化的数据转换为低维的、连续的向量的一种技术。因为计算机无法直接理解现实的事物，为了让计算机能够理解，我们就需要将现实的事物描述成一组向量，Embedding 就是现实世界和计算机之间的桥梁。现实世界中越相似的事物在转换成向量之后，它们的位置也越接近。

Embedding 之后的向量不仅满足向量的数学性质（可计算距离、相似度），还承载了现实事物之间的关系。

#### Embedding 的过程

![[embedding过程.png]]

以文本句子的 embedding 为例。

1. 首先对输入进行清洗，去除一些无意义的符号以及进行大小写转换等等。

2. 如果我们的序列长度是 1024 ，接下来需要对输入增加一些有特殊含义的字符，比如 `<sos>`、`<eos>` 等，如果输入长度不足 1024 ，需要再末尾添加 `<pad>` ，最终得到一个 1024 个元素的向量，最终得到的序列像这样 `<sos>who am I<eos><pad><pad>...<pad>` 。

3. 将处理好的输入序列中的每个元素根据一个词表 Vocab 的映射得到一个整数，最终将输入序列转化成一组向量，比如这样：`[777, 1000, 2000, 3000, 888, 999, ..., 999]` ，这里的 ID 是假设的。

4. 每个模型都有自己对应的 Embedding 矩阵，如果我们的词表大小是 10w ，特征维度是 768 ，那么 Embedding 矩阵的形状就是 `10w x 768`。我们将原始向量中的每一个 TokenId 拿到 Embedding 矩阵中查找对应的行，比如 777 就查找第 777 行，查询的结果就是一个 768 维的向量。最终我们就得到了一个形状为 `1024 x 768` 的矩阵。

5. 我们现在得到了一个包含语义的矩阵，但是在进行矩阵运算时无法保证元素的先后顺序，所以模型还有一个位置编码矩阵，它的大小也是 `10w x 768`，我们根据索引位置查询对应的行，比如第 0 个 token，就查询位置编码矩阵的第 0 行，最终生成形状为 `1024 x 768` 的矩阵。然后我们将这 2 个矩阵相加，最终得到的矩阵就既保护了语义信息，又包含了位置信息。这个矩阵就会作为输入交给模型来处理。
 



#### Embedding 模型是怎么训练出来的

Embedding 模型的训练主要使用无监督学习，训练方法很多，常用的比如对比学习（Contrastive Learning）。

对比学习就是通过正样本和负样本让模型学习，比如给模型很多狗的图片告诉它这是狗，给模型很多猫的图片告诉它这不是狗。最终的目标是让模型在处理时将数据和正样本的距离最小化，和负样本的距离最大化。

训练流程通常分为 2 阶段：
1. pretrain 阶段
	使用海量数据（如互联网网页，书籍等）让模型学习通用知识和能力。

2. fine-tuning 阶段
	使用少量的标注数据，让模型在与训练的基础上进行微调，使其具有适合特定任务的能力，如生成、检索等。

### 向量数据库

向量（Vector Database）数据库就是用来存储和查询向量的数据库，它的设计目的是进行语义相似度搜索。向量数据库存储向量和原始文本信息，通过相似度查询找到最近的向量，然后返回对应的文本。



### RAG 的流程

RAG 的流程分成 2 个阶段：
1. 数据准备阶段：索引 - 嵌入 - 存储
2. 运行阶段：提问 - 检索 - 增强 - 生成

![[rag.png]]
#### 索引 - 嵌入 - 存储

将非结构化的文本数据分成更小的、有意义的文本块，因为 LLM 有上下文限制，小的文本更容易检索。然后使用 Embedding 模型将分片的文本 embedding 成向量存到数据库。

#### 检索

检索也叫召回，是搜索与用户问题相关的文本的过程。检索是通过相似度查询，返回 Top K 个文档片段。检索类似海选。

#### 增强

增强就是在检索返回的基础上使用更复杂的方式（比如使用交叉编码器 Cross-Encoder）对结果进行比较，找到最接近的几个文本，将文本构造成 LLM 的 Prompt 。增强类似精选。

#### 生成

生成就是 LLM 拿到增强阶段生成的 Prompt 生成最终的输出结果返回给用户。

### 相似度搜索